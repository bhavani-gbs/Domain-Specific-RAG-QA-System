{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9db155e7",
   "metadata": {},
   "source": [
    "# ğŸ” Domain-Specific Q&A Using RAG (Retrieval-Augmented Generation)\n",
    "## AI/Machine Learning Domain\n",
    "\n",

    "**Course:** Natural Language Processing  \n",
    "**Date:** February 2026  \n",
    "**Runtime:** Google Colab (Free Tier)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eead52f",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "### Objective\n",
    "Build a complete **Retrieval-Augmented Generation (RAG)** pipeline for answering technical questions in the **AI/Machine Learning** domain. The system ingests academic PDFs, creates a searchable vector store, retrieves relevant passages for a given query, and uses a language model to generate accurate, context-grounded answers.\n",
    "\n",
    "### Why RAG?\n",
    "Standard language models generate answers from their parametric memory (training data), which can lead to hallucinations or outdated information. RAG addresses this by **retrieving relevant documents first**, then feeding them as context to the generator â€” grounding the answer in actual source material.\n",
    "\n",
    "### Domain & Knowledge Base\n",
    "We use **6 PDF documents** covering core AI/ML topics:\n",
    "\n",
    "| # | Document | Description |\n",
    "|---|----------|-------------|\n",
    "| 1 | `attention_is_all_you_need.pdf` | Original Transformer paper (Vaswani et al., 2017) |\n",
    "| 2 | `cnn_transformers_intro.pdf` | Introduction to CNNs and Transformers |\n",
    "| 3 | `cs224n_merged_notes.pdf` | Stanford CS224N (NLP) merged lecture notes |\n",
    "| 4 | `cs224n_transformers_2024.pdf` | CS224N Transformers lecture (2024 edition) |\n",
    "| 5 | `cs231n_full_notes.pdf` | Stanford CS231N (Computer Vision) full notes |\n",
    "| 6 | `neural_networks_backprop.pdf` | Neural networks & backpropagation notes |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34036bc8",
   "metadata": {},
   "source": [
    "### Pipeline Architecture\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  6 AI/ML    â”‚â”€â”€â”€â–¶â”‚ Text Extract â”‚â”€â”€â”€â–¶â”‚  Chunking  â”‚â”€â”€â”€â–¶â”‚  Embedding     â”‚\n",
    "â”‚  PDFs       â”‚    â”‚ + Cleaning   â”‚    â”‚ (500 tok,  â”‚    â”‚ (MiniLM-L6-v2) â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ 50 overlap)â”‚    â”‚  384-dim       â”‚\n",
    "                                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                                                 â”‚\n",
    "                                                                 â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Generated  â”‚â—€â”€â”€â”€â”‚  FLAN-T5     â”‚â—€â”€â”€â”€â”‚  Prompt    â”‚â—€â”€â”€â”€â”‚  FAISS Index   â”‚\n",
    "â”‚  Answer     â”‚    â”‚  -large      â”‚    â”‚  Building  â”‚    â”‚  (Flat L2)     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                            â–²\n",
    "                                            â”‚\n",
    "                                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                                       â”‚  User      â”‚\n",
    "                                       â”‚  Query     â”‚\n",
    "                                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Flow:** PDFs â†’ Extract Text â†’ Clean â†’ Chunk â†’ Embed â†’ Store in FAISS â†’ User asks question â†’ Embed query â†’ Retrieve top-k chunks â†’ Build prompt with context â†’ FLAN-T5-large generates answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8924b3cd",
   "metadata": {},
   "source": [
    "## 2. Methodology\n",
    "\n",
    "### Pipeline Stages\n",
    "\n",
    "Our RAG system operates in **7 sequential stages**:\n",
    "\n",
    "1. **PDF Text Extraction** â€” Read raw text from each PDF page using PyPDF2\n",
    "2. **Text Cleaning** â€” Remove excess whitespace, fix encoding artifacts, normalize formatting\n",
    "3. **Chunking** â€” Split cleaned text into overlapping chunks (~500 tokens, 50 token overlap) with source metadata\n",
    "4. **Embedding Generation** â€” Encode each chunk into a 384-dimensional dense vector using `all-MiniLM-L6-v2`\n",
    "5. **FAISS Indexing** â€” Store all embeddings in a FAISS flat L2 index for exact nearest-neighbor search\n",
    "6. **Retrieval** â€” Given a user query, embed it and retrieve the top-k most similar chunks from the index\n",
    "7. **Generation** â€” Feed retrieved chunks as context into a prompt template, then generate an answer using FLAN-T5-large\n",
    "\n",
    "### Model Choices & Justification\n",
    "\n",
    "| Component | Model / Tool | Justification |\n",
    "|-----------|-------------|---------------|\n",
    "| Embeddings | `all-MiniLM-L6-v2` | Fast, 384-dim, excellent quality-to-size ratio, fits Colab free tier |\n",
    "| Vector Store | FAISS `IndexFlatL2` | Exact search (no approximation error), dataset is small enough, simple to explain |\n",
    "| Generator | `google/flan-t5-large` (~780M params) | Instruction-tuned, runs on T4 GPU, good at following prompt templates |\n",
    "| PDF Parsing | PyPDF2 | Lightweight, no system dependencies, handles standard academic PDFs |\n",
    "\n",
    "### Evaluation Plan\n",
    "\n",
    "We compare **RAG answers** vs. **Baseline answers** (no retrieval) on 8â€“10 domain-specific questions using:\n",
    "\n",
    "- **ROUGE-L** â€” Measures longest common subsequence overlap with reference answers (lexical similarity)\n",
    "- **Cosine Semantic Similarity** â€” Uses MiniLM embeddings to measure meaning-level similarity with reference answers\n",
    "\n",
    "This dual evaluation captures both surface-level word overlap and deeper semantic alignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b221dc",
   "metadata": {},
   "source": [
    "## 3. Implementation\n",
    "\n",
    "### 3A. Environment Setup\n",
    "\n",
    "Mount Google Drive and install all required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "45744e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "27910e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project directory created at: /content/drive/MyDrive/rag_project\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_PATH = \"/content/drive/MyDrive/rag_project\"\n",
    "DATA_PATH = os.path.join(PROJECT_PATH, \"data\")\n",
    "\n",
    "os.makedirs(DATA_PATH, exist_ok=True)\n",
    "\n",
    "print(\"Project directory created at:\", PROJECT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c0c35d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in data folder:\n",
      "['cs231n_full_notes.pdf', 'cnn_transformers_intro.pdf', 'cs224n_transformers_2024.pdf', 'attention_is_all_you_need.pdf', 'cs224n_merged_notes.pdf', 'neural_networks_backprop.pdf']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"Files in data folder:\")\n",
    "print(os.listdir(DATA_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e1ef4bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q PyPDF2 sentence-transformers faiss-cpu transformers rouge-score torch pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f22a2cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from rouge_score import rouge_scorer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7fbcb91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages loaded: 594\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "\n",
    "for file in os.listdir(DATA_PATH):\n",
    "    if file.endswith(\".pdf\"):\n",
    "        reader = PdfReader(os.path.join(DATA_PATH, file))\n",
    "        for page_num, page in enumerate(reader.pages):\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                documents.append({\n",
    "                    \"source\": file,\n",
    "                    \"page\": page_num,\n",
    "                    \"text\": text\n",
    "                })\n",
    "\n",
    "print(\"Total pages loaded:\", len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5fdeb8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample cleaned text:\n",
      "\n",
      "DEEP LEARNING STUDY NOTES All credits go to L. Fei-Fei, A. Karpathy, J.Johnson teachers of the CS231n course. Thank you for this amazing course!! by Albert Pumarola\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'\\n+', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'-\\s+', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "for doc in documents:\n",
    "    doc[\"text\"] = clean_text(doc[\"text\"])\n",
    "\n",
    "print(\"Sample cleaned text:\\n\")\n",
    "print(documents[0][\"text\"][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "888aed9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 674\n"
     ]
    }
   ],
   "source": [
    "def chunk_text(text, chunk_size=400, overlap=50):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "\n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk = \" \".join(words[i:i + chunk_size])\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "all_chunks = []\n",
    "chunk_metadata = []\n",
    "\n",
    "for doc in documents:\n",
    "    chunks = chunk_text(doc[\"text\"])\n",
    "    for chunk in chunks:\n",
    "        all_chunks.append(chunk)\n",
    "        chunk_metadata.append({\n",
    "            \"source\": doc[\"source\"],\n",
    "            \"page\": doc[\"page\"]\n",
    "        })\n",
    "\n",
    "print(\"Total chunks created:\", len(all_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e6b2b3cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca408c9e42ce465c833932e491407f54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67d1db0219f748a1b7d0058823f2da7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (674, 384)\n"
     ]
    }
   ],
   "source": [
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "chunk_embeddings = embedding_model.encode(\n",
    "    all_chunks,\n",
    "    batch_size=32,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True\n",
    ")\n",
    "\n",
    "print(\"Embedding matrix shape:\", chunk_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4e6d2df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vectors in index: 674\n"
     ]
    }
   ],
   "source": [
    "dimension = chunk_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(chunk_embeddings)\n",
    "\n",
    "print(\"Total vectors in index:\", index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6707f3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, top_k=5):\n",
    "    query_embedding = embedding_model.encode([query], convert_to_numpy=True)\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "\n",
    "    results = []\n",
    "    for idx in indices[0]:\n",
    "        results.append({\n",
    "            \"chunk\": all_chunks[idx],\n",
    "            \"source\": chunk_metadata[idx]\n",
    "        })\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "89824cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8958831e53404c2a990bff11f74dfe5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/558 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tied weights mapping and config for this model specifies to tie shared.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator loaded on: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\").to(device)\n",
    "\n",
    "print(\"Generator loaded on:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fb25e50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You are an AI assistant answering questions about AI and Machine Learning.\n",
    "\n",
    "Use the provided context to answer the question clearly and completely.\n",
    "Write a short explanatory paragraph (3-5 sentences).\n",
    "Do not answer with a single word.\n",
    "Do not copy raw fragments.\n",
    "Explain the concept in your own words using the context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6035c8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (546 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ RAG generation function defined\n",
      "  Test prompt: 546 tokens (limit: 1024)\n",
      "  Fits: YES âœ“\n",
      "\n",
      "--- Prompt preview (first 400 chars) ---\n",
      "\n",
      "You are an AI assistant answering questions about AI and Machine Learning.\n",
      "\n",
      "Use the provided context to answer the question clearly and completely.\n",
      "Write a short explanatory paragraph (3-5 sentences).\n",
      "Do not answer with a single word.\n",
      "Do not copy raw fragments.\n",
      "Explain the concept in your own words using the context.\n",
      "\n",
      "Context:\n",
      "Attention is weighted averaging, which lets you do lookups! 11 Attenti\n"
     ]
    }
   ],
   "source": [
    "def build_rag_prompt(question, retrieved_docs, max_context_tokens=800):\n",
    "    \"\"\"\n",
    "    Build a prompt that fits within FLAN-T5-large's 1024-token input limit.\n",
    "    Uses actual tokenizer for precise token-level truncation.\n",
    "    \"\"\"\n",
    "    context_parts = []\n",
    "    used_tokens = 0\n",
    "\n",
    "    for doc in retrieved_docs:\n",
    "        chunk_text = doc[\"chunk\"]\n",
    "        chunk_ids = tokenizer.encode(chunk_text, add_special_tokens=False)\n",
    "\n",
    "        if used_tokens + len(chunk_ids) <= max_context_tokens:\n",
    "            context_parts.append(chunk_text)\n",
    "            used_tokens += len(chunk_ids)\n",
    "        else:\n",
    "            remaining = max_context_tokens - used_tokens\n",
    "            if remaining > 30:\n",
    "                truncated_text = tokenizer.decode(chunk_ids[:remaining], skip_special_tokens=True)\n",
    "                context_parts.append(truncated_text)\n",
    "                used_tokens += remaining\n",
    "            break\n",
    "\n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "    prompt = PROMPT_TEMPLATE.format(context=context, question=question)\n",
    "    return prompt\n",
    "\n",
    "\n",
    "GENERATION_CONFIG = {\n",
    "    \"max_new_tokens\": 180,\n",
    "    \"min_new_tokens\": 40,\n",
    "    \"num_beams\": 4,\n",
    "    \"length_penalty\": 1.1,\n",
    "    \"no_repeat_ngram_size\": 3,\n",
    "    \"early_stopping\": True,\n",
    "}\n",
    "\n",
    "\n",
    "def generate_rag_answer(question, top_k=5):\n",
    "    \"\"\"RAG pipeline: Retrieve â†’ Build token-aware prompt â†’ Generate.\"\"\"\n",
    "    retrieved_docs = retrieve(question, top_k)\n",
    "    prompt = build_rag_prompt(question, retrieved_docs)\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
    "    outputs = model.generate(**inputs, **GENERATION_CONFIG)\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return answer, retrieved_docs\n",
    "\n",
    "\n",
    "# Quick check: verify prompt fits\n",
    "test_retrieved = retrieve(\"What is attention?\", top_k=5)\n",
    "test_prompt = build_rag_prompt(\"What is attention?\", test_retrieved)\n",
    "test_tokens = len(tokenizer.encode(test_prompt))\n",
    "print(f\"âœ“ RAG generation function defined\")\n",
    "print(f\"  Test prompt: {test_tokens} tokens (limit: 1024)\")\n",
    "print(f\"  Fits: {'YES âœ“' if test_tokens <= 1024 else 'NO âœ—'}\")\n",
    "print(f\"\\n--- Prompt preview (first 400 chars) ---\")\n",
    "print(test_prompt[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8d44ad32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Baseline generation function defined\n"
     ]
    }
   ],
   "source": [
    "def generate_baseline_answer(question):\n",
    "    \"\"\"Baseline: Generate answer WITHOUT retrieval (parametric knowledge only).\"\"\"\n",
    "    prompt = f\"Answer the question clearly in 3-5 sentences.\\n\\nQuestion:\\n{question}\\n\\nAnswer:\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
    "    outputs = model.generate(**inputs, **GENERATION_CONFIG)\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return answer\n",
    "\n",
    "print(\"âœ“ Baseline generation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cf47b2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "QUESTION: What is the key innovation of the Transformer architecture?\n",
      "======================================================================\n",
      "\n",
      "BASELINE ANSWER (no retrieval):\n",
      "  This is the key innovation of the Transformer architecture. Therefore the final question is: what are the key innovations of the transformer architecture?. It's impossible to answer this question because there are multiple possible answers. The final answer may not be the best one for your situation.\n",
      "\n",
      "RAG ANSWER (with retrieval):\n",
      "  Parallelizability allows for efficient pretraining, and have made them the de-facto standard. On this popular aggregate benchmark, for example: All top models are Transformer (and pretraining)-based. More results Thursday when we discuss pretraining.\n",
      "\n",
      "SOURCES: [{'source': 'cs224n_transformers_2024.pdf', 'page': 68}, {'source': 'cs224n_transformers_2024.pdf', 'page': 62}, {'source': 'cs224n_transformers_2024.pdf', 'page': 52}]\n",
      "\n",
      "======================================================================\n",
      "QUESTION: What is backpropagation?\n",
      "======================================================================\n",
      "\n",
      "BASELINE ANSWER (no retrieval):\n",
      "  Backpropagation is a technique used by computer programmers to improve the performance of a computer program. Therefore the final answer is backpropagating .  Backpro propagation '' .\n",
      "\n",
      "RAG ANSWER (with retrieval):\n",
      "  Backpropagation is a way of computing gradients of expressions through recursive application ofchain rule . Understanding this process and its subtleties is critical for you to understand, and e ectively develop, design and debug Neural Networks .\n",
      "\n",
      "SOURCES: [{'source': 'neural_networks_backprop.pdf', 'page': 4}, {'source': 'neural_networks_backprop.pdf', 'page': 58}, {'source': 'cs231n_full_notes.pdf', 'page': 32}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demo_questions = [\n",
    "    \"What is the key innovation of the Transformer architecture?\",\n",
    "    \"What is backpropagation?\"\n",
    "]\n",
    "\n",
    "for question in demo_questions:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"QUESTION:\", question)\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    baseline_answer = generate_baseline_answer(question)\n",
    "    rag_answer, sources = generate_rag_answer(question)\n",
    "\n",
    "    print(f\"\\nBASELINE ANSWER (no retrieval):\\n  {baseline_answer}\")\n",
    "    print(f\"\\nRAG ANSWER (with retrieval):\\n  {rag_answer}\")\n",
    "    print(f\"\\nSOURCES: {[s['source'] for s in sources[:3]]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a0b3b216",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "\n",
    "def compute_rouge(pred, ref):\n",
    "    return scorer.score(ref, pred)['rougeL'].fmeasure\n",
    "\n",
    "def compute_cosine(pred, ref):\n",
    "    emb = embedding_model.encode([pred, ref], convert_to_numpy=True)\n",
    "    return float(cosine_similarity([emb[0]], [emb[1]])[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a9694648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Defined 10 test questions with reference answers\n",
      "  Q1: What is the key innovation of the Transformer architecture?...\n",
      "  Q2: How does multi-head attention work in Transformers?...\n",
      "  Q3: What is backpropagation?...\n",
      "  Q4: What is the purpose of positional encoding in Transformers?...\n",
      "  Q5: What is the vanishing gradient problem?...\n",
      "  Q6: How do Convolutional Neural Networks process images?...\n",
      "  Q7: What is the softmax function and where is it used in attenti...\n",
      "  Q8: What is dropout regularization and why is it used?...\n",
      "  Q9: What is the difference between self-attention and cross-atte...\n",
      "  Q10: How does the encoder-decoder architecture work in sequence-t...\n"
     ]
    }
   ],
   "source": [
    "test_data = [\n",
    "    {\n",
    "        \"question\": \"What is the key innovation of the Transformer architecture?\",\n",
    "        \"reference\": \"The key innovation of the Transformer is the self-attention mechanism, which allows the model to process all positions in a sequence simultaneously rather than sequentially, eliminating the need for recurrence and enabling better parallelization.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How does multi-head attention work in Transformers?\",\n",
    "        \"reference\": \"Multi-head attention runs multiple attention functions in parallel, each with different learned linear projections of queries, keys, and values. The outputs from each head are concatenated and linearly projected to produce the final output.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is backpropagation?\",\n",
    "        \"reference\": \"Backpropagation is an algorithm for computing gradients of the loss function with respect to each weight in a neural network by applying the chain rule of calculus, propagating the error signal backward from the output layer through hidden layers.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the purpose of positional encoding in Transformers?\",\n",
    "        \"reference\": \"Positional encoding injects information about the position of tokens in the sequence since the Transformer has no inherent notion of order. The original paper uses sinusoidal functions of different frequencies added to the input embeddings.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the vanishing gradient problem?\",\n",
    "        \"reference\": \"The vanishing gradient problem occurs when gradients become extremely small as they propagate backward through many layers of a deep neural network, making it difficult for earlier layers to learn effectively.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How do Convolutional Neural Networks process images?\",\n",
    "        \"reference\": \"CNNs process images by applying learnable convolutional filters across the input to detect local features like edges and textures. Through successive convolutional and pooling layers, the network builds hierarchical feature representations.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the softmax function and where is it used in attention?\",\n",
    "        \"reference\": \"The softmax function converts raw scores into a probability distribution by exponentiating each score and normalizing. In attention mechanisms, softmax is applied to the query-key dot products to produce attention weights that determine how much each value contributes.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is dropout regularization and why is it used?\",\n",
    "        \"reference\": \"Dropout is a regularization technique where randomly selected neurons are temporarily removed during training. This prevents co-adaptation and forces the network to learn more robust features, reducing overfitting.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the difference between self-attention and cross-attention?\",\n",
    "        \"reference\": \"In self-attention, queries, keys, and values all come from the same sequence. In cross-attention, queries come from one sequence while keys and values come from another, enabling the decoder to attend to the encoder output.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How does the encoder-decoder architecture work in sequence-to-sequence models?\",\n",
    "        \"reference\": \"The encoder processes the input sequence and produces hidden representations. The decoder generates output tokens one at a time, using cross-attention to attend to the encoder states. This handles inputs and outputs of different lengths.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"âœ“ Defined {len(test_data)} test questions with reference answers\")\n",
    "for i, item in enumerate(test_data):\n",
    "    print(f\"  Q{i+1}: {item['question'][:60]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1215e63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Q1/10: What is the key innovation of the Transformer arch...\n",
      "Processing Q2/10: How does multi-head attention work in Transformers...\n",
      "Processing Q3/10: What is backpropagation?...\n",
      "Processing Q4/10: What is the purpose of positional encoding in Tran...\n",
      "Processing Q5/10: What is the vanishing gradient problem?...\n",
      "Processing Q6/10: How do Convolutional Neural Networks process image...\n",
      "Processing Q7/10: What is the softmax function and where is it used ...\n",
      "Processing Q8/10: What is dropout regularization and why is it used?...\n",
      "Processing Q9/10: What is the difference between self-attention and ...\n",
      "Processing Q10/10: How does the encoder-decoder architecture work in ...\n",
      "\n",
      "âœ“ All 10 questions processed!\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "question",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "rag_rouge",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "baseline_rouge",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "rag_cosine",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "baseline_cosine",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "c138d14c-2f3e-414e-9123-c94daed64cc1",
       "rows": [
        [
         "0",
         "What is the key innovation of the Transformer architecture?",
         "0.11428571428571428",
         "0.2891566265060241",
         "0.5129251480102539",
         "0.5439249277114868"
        ],
        [
         "1",
         "How does multi-head attention work in Transformers?",
         "0.2",
         "0.2058823529411765",
         "0.5322343707084656",
         "0.5812491178512573"
        ],
        [
         "2",
         "What is backpropagation?",
         "0.2162162162162162",
         "0.22222222222222224",
         "0.8460710048675537",
         "0.7927349209785461"
        ],
        [
         "3",
         "What is the purpose of positional encoding in Transformers?",
         "0.16129032258064516",
         "0.22950819672131148",
         "0.7009307146072388",
         "0.6655086278915405"
        ],
        [
         "4",
         "What is the vanishing gradient problem?",
         "0.1764705882352941",
         "0.19999999999999998",
         "0.801975667476654",
         "0.8175402879714966"
        ],
        [
         "5",
         "How do Convolutional Neural Networks process images?",
         "0.20512820512820512",
         "0.1038961038961039",
         "0.5278527736663818",
         "-0.04354667663574219"
        ],
        [
         "6",
         "What is the softmax function and where is it used in attention?",
         "0.2898550724637681",
         "0.22222222222222224",
         "0.7318608164787292",
         "0.716505765914917"
        ],
        [
         "7",
         "What is dropout regularization and why is it used?",
         "0.16279069767441862",
         "0.15584415584415587",
         "0.7946232557296753",
         "0.7028422951698303"
        ],
        [
         "8",
         "What is the difference between self-attention and cross-attention?",
         "0.1956521739130435",
         "0.16216216216216217",
         "0.5548925399780273",
         "0.31057119369506836"
        ],
        [
         "9",
         "How does the encoder-decoder architecture work in sequence-to-sequence models?",
         "0.30769230769230765",
         "0.2333333333333333",
         "0.636457085609436",
         "0.6343626976013184"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 10
       }
      },
      "text/html": [
       "\n",
       "  <div id=\"df-83793b45-2238-4151-8bbb-de73c0f7caef\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>rag_rouge</th>\n",
       "      <th>baseline_rouge</th>\n",
       "      <th>rag_cosine</th>\n",
       "      <th>baseline_cosine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the key innovation of the Transformer architecture?</td>\n",
       "      <td>0.114286</td>\n",
       "      <td>0.289157</td>\n",
       "      <td>0.512925</td>\n",
       "      <td>0.543925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How does multi-head attention work in Transformers?</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.205882</td>\n",
       "      <td>0.532234</td>\n",
       "      <td>0.581249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is backpropagation?</td>\n",
       "      <td>0.216216</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.846071</td>\n",
       "      <td>0.792735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the purpose of positional encoding in Transformers?</td>\n",
       "      <td>0.161290</td>\n",
       "      <td>0.229508</td>\n",
       "      <td>0.700931</td>\n",
       "      <td>0.665509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the vanishing gradient problem?</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.801976</td>\n",
       "      <td>0.817540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How do Convolutional Neural Networks process images?</td>\n",
       "      <td>0.205128</td>\n",
       "      <td>0.103896</td>\n",
       "      <td>0.527853</td>\n",
       "      <td>-0.043547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What is the softmax function and where is it used in attention?</td>\n",
       "      <td>0.289855</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.731861</td>\n",
       "      <td>0.716506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What is dropout regularization and why is it used?</td>\n",
       "      <td>0.162791</td>\n",
       "      <td>0.155844</td>\n",
       "      <td>0.794623</td>\n",
       "      <td>0.702842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What is the difference between self-attention and cross-attention?</td>\n",
       "      <td>0.195652</td>\n",
       "      <td>0.162162</td>\n",
       "      <td>0.554893</td>\n",
       "      <td>0.310571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How does the encoder-decoder architecture work in sequence-to-sequence models?</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.636457</td>\n",
       "      <td>0.634363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "      \n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-83793b45-2238-4151-8bbb-de73c0f7caef')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "      \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-83793b45-2238-4151-8bbb-de73c0f7caef button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-83793b45-2238-4151-8bbb-de73c0f7caef');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "  \n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                                                         question  \\\n",
       "0                     What is the key innovation of the Transformer architecture?   \n",
       "1                             How does multi-head attention work in Transformers?   \n",
       "2                                                        What is backpropagation?   \n",
       "3                     What is the purpose of positional encoding in Transformers?   \n",
       "4                                         What is the vanishing gradient problem?   \n",
       "5                            How do Convolutional Neural Networks process images?   \n",
       "6                 What is the softmax function and where is it used in attention?   \n",
       "7                              What is dropout regularization and why is it used?   \n",
       "8              What is the difference between self-attention and cross-attention?   \n",
       "9  How does the encoder-decoder architecture work in sequence-to-sequence models?   \n",
       "\n",
       "   rag_rouge  baseline_rouge  rag_cosine  baseline_cosine  \n",
       "0   0.114286        0.289157    0.512925         0.543925  \n",
       "1   0.200000        0.205882    0.532234         0.581249  \n",
       "2   0.216216        0.222222    0.846071         0.792735  \n",
       "3   0.161290        0.229508    0.700931         0.665509  \n",
       "4   0.176471        0.200000    0.801976         0.817540  \n",
       "5   0.205128        0.103896    0.527853        -0.043547  \n",
       "6   0.289855        0.222222    0.731861         0.716506  \n",
       "7   0.162791        0.155844    0.794623         0.702842  \n",
       "8   0.195652        0.162162    0.554893         0.310571  \n",
       "9   0.307692        0.233333    0.636457         0.634363  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for i, item in enumerate(test_data):\n",
    "    q = item[\"question\"]\n",
    "    ref = item[\"reference\"]\n",
    "\n",
    "    print(f\"Processing Q{i+1}/{len(test_data)}: {q[:50]}...\")\n",
    "\n",
    "    rag_ans, sources = generate_rag_answer(q)\n",
    "    base_ans = generate_baseline_answer(q)\n",
    "\n",
    "    results.append({\n",
    "        \"question\": q,\n",
    "        \"reference\": ref,\n",
    "        \"rag_answer\": rag_ans,\n",
    "        \"baseline_answer\": base_ans,\n",
    "        \"rag_rouge\": compute_rouge(rag_ans, ref),\n",
    "        \"baseline_rouge\": compute_rouge(base_ans, ref),\n",
    "        \"rag_cosine\": compute_cosine(rag_ans, ref),\n",
    "        \"baseline_cosine\": compute_cosine(base_ans, ref),\n",
    "        \"top_source\": sources[0][\"source\"] if sources else \"N/A\"\n",
    "    })\n",
    "\n",
    "print(f\"\\nâœ“ All {len(results)} questions processed!\")\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df[[\"question\", \"rag_rouge\", \"baseline_rouge\", \"rag_cosine\", \"baseline_cosine\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "adfd5c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_retrieval_details(question, top_k=5):\n",
    "    retrieved = retrieve(question, top_k)\n",
    "\n",
    "    print(\"QUESTION:\", question)\n",
    "    print(\"\\nTop Retrieved Chunks:\\n\")\n",
    "\n",
    "    for i, doc in enumerate(retrieved):\n",
    "        print(f\"--- Rank {i+1} ---\")\n",
    "        print(\"Source:\", doc[\"source\"])\n",
    "        print(\"Preview:\", doc[\"chunk\"][:300], \"...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f6eb250f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION: What is backpropagation?\n",
      "\n",
      "Top Retrieved Chunks:\n",
      "\n",
      "--- Rank 1 ---\n",
      "Source: {'source': 'neural_networks_backprop.pdf', 'page': 4}\n",
      "Preview: Fei-Fei Li, Ehsan AdeliLecture 4 April 11, 2024Administrative: Discussion SectionDiscussion section tomorrow (led by Lucas Leanza):Backpropagation 5 ...\n",
      "\n",
      "--- Rank 2 ---\n",
      "Source: {'source': 'neural_networks_backprop.pdf', 'page': 58}\n",
      "Preview: Fei-Fei Li, Ehsan AdeliLecture 4 April 11, 202460 Backpropagation: a simple example ...\n",
      "\n",
      "--- Rank 3 ---\n",
      "Source: {'source': 'cs231n_full_notes.pdf', 'page': 32}\n",
      "Preview: Chapter 7 Backpropagation Introduction Motivation In this section we will develop expertise with an intuitive understanding of backpropagation, which is a way of computing gradients of expressions through recursive application ofchain rule . Understanding of this process and its subtleties is critic ...\n",
      "\n",
      "--- Rank 4 ---\n",
      "Source: {'source': 'neural_networks_backprop.pdf', 'page': 0}\n",
      "Preview: Fei-Fei Li, Ehsan AdeliLecture 4 April 11, 20241Lecture 4:Neural Networks and Backpropagation ...\n",
      "\n",
      "--- Rank 5 ---\n",
      "Source: {'source': 'cs231n_full_notes.pdf', 'page': 35}\n",
      "Preview: forward, we will want to use a more concise notation so that we dont have to keep writing the dfpart. That is, for example instead of dfdq we would simply write dq, and always assume that the gradient is with respect to the nal output. Figure 7.1: Neuron backprop This computation can also be nicely  ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_retrieval_details(\"What is backpropagation?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "234a2df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "         RAG vs BASELINE â€” ANSWER COMPARISON\n",
      "================================================================================\n",
      "\n",
      "Q1: What is the key innovation of the Transformer architecture?\n",
      "  Baseline: This is the key innovation of the Transformer architecture. Therefore the final question is: what are the key innovations of the transformer architecture?. It's impossible to answer this question because there are multiple possible answers. The final answer may not be the best one for your situation.\n",
      "  RAG:      Parallelizability allows for efficient pretraining, and have made them the de-facto standard. On this popular aggregate benchmark, for example: All top models are Transformer (and pretraining)-based. More results Thursday when we discuss pretraining.\n",
      "  Source:   {'source': 'cs224n_transformers_2024.pdf', 'page': 68}\n",
      "  ROUGE-L:  baseline=0.2892  RAG=0.1143\n",
      "  Cosine:   baseline=0.5439  RAG=0.5129\n",
      "\n",
      "Q2: How does multi-head attention work in Transformers?\n",
      "  Baseline: Multi-Head Attenuation works in Transformers by focusing on one head at a time. Therefore the final answer should be multi-head attention focuses on more than one head. The final question is:\n",
      "  RAG:      In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values comes from the output of the encoder. This allows every position in the decode to attend over all positions in the input sequence This mimics the typical encoder- decode attention mechanisms in sequence-to-sequence models\n",
      "  Source:   {'source': 'cs224n_transformers_2024.pdf', 'page': 47}\n",
      "  ROUGE-L:  baseline=0.2059  RAG=0.2000\n",
      "  Cosine:   baseline=0.5812  RAG=0.5322\n",
      "\n",
      "Q3: What is backpropagation?\n",
      "  Baseline: Backpropagation is a technique used by computer programmers to improve the performance of a computer program. Therefore the final answer is backpropagating .  Backpro propagation '' .\n",
      "  RAG:      Backpropagation is a way of computing gradients of expressions through recursive application ofchain rule . Understanding this process and its subtleties is critical for you to understand, and e ectively develop, design and debug Neural Networks .\n",
      "  Source:   {'source': 'neural_networks_backprop.pdf', 'page': 4}\n",
      "  ROUGE-L:  baseline=0.2222  RAG=0.2162\n",
      "  Cosine:   baseline=0.7927  RAG=0.8461\n",
      "\n",
      "Q4: What is the purpose of positional encoding in Transformers?\n",
      "  Baseline: Positional encoding in Transformers is used to store information in a database. Therefore, the final answer should be positional coding in transformers . '' Positional Encoding In Transformers '.\n",
      "  RAG:      Positional encoding in Transformers is a way of enforcing a position that isnâ€™t as important as it would be if absolute position isn't so important.\n",
      "  Source:   {'source': 'cnn_transformers_intro.pdf', 'page': 84}\n",
      "  ROUGE-L:  baseline=0.2295  RAG=0.1613\n",
      "  Cosine:   baseline=0.6655  RAG=0.7009\n",
      "\n",
      "Q5: What is the vanishing gradient problem?\n",
      "  Baseline: The vanishing gradient problem refers to a situation in which there is a decrease in gradient over a long period of time. The answer: vanizing gradient problem .  vanishing gradient ''.\n",
      "  RAG:      The vanishing-gradient problem refers to the fact that if a local gradient disappears, the upstream and downstream gradients disappear at the same time. This problem can be solved by reversing the direction of the local gradient.\n",
      "  Source:   {'source': 'neural_networks_backprop.pdf', 'page': 74}\n",
      "  ROUGE-L:  baseline=0.2000  RAG=0.1765\n",
      "  Cosine:   baseline=0.8175  RAG=0.8020\n",
      "\n",
      "Q6: How do Convolutional Neural Networks process images?\n",
      "  Baseline: None of the above choices is the best answer for the above question . It is impossible to answer this question accurately without describing it in sufficient detail and providing sufficient context to allow the reader to draw his or her own conclusions based on this information.\n",
      "  RAG:      They pass the input image through a CNN which produces the low res image segmentation (labels). 2. Rescale the inputs to the lower resolution and apply again the ConvNet to the higher resolution input image and results from the previous segmentation 3. Goto 2 until threshold Upsampling.\n",
      "  Source:   {'source': 'cs231n_full_notes.pdf', 'page': 150}\n",
      "  ROUGE-L:  baseline=0.1039  RAG=0.2051\n",
      "  Cosine:   baseline=-0.0435  RAG=0.5279\n",
      "\n",
      "Q7: What is the softmax function and where is it used in attention?\n",
      "  Baseline: The softmax function is a function used in attention. It can be found in the hippocampus . It is used in the prefrontal lobules of the brain . So the final question is: What is thesoftmax function and where is it used in Attention?\n",
      "  RAG:      The softmax function can be used to normalize scalar scores into a vector ai= (ai,1, .. . , a i, n), using a softmax layer. The vector is called the attention vector.\n",
      "  Source:   {'source': 'cnn_transformers_intro.pdf', 'page': 62}\n",
      "  ROUGE-L:  baseline=0.2222  RAG=0.2899\n",
      "  Cosine:   baseline=0.7165  RAG=0.7319\n",
      "\n",
      "Q8: What is dropout regularization and why is it used?\n",
      "  Baseline: Dropout regularization is a technique used to reduce drop-out rates. Drop-out regularisation can be used as a tool to reduce student drop-off rates. It can also be used to increase student retention rates. Therefore, drop- out regularization should not be used unless it is necessary.\n",
      "  RAG:      Dropout is a powerful technique for regularization, rst introduced by Srivastava et al. in Dropout: A Simple Way to Prevent Neural Networks from Overtting. The idea is simple yet effective â€“ during training, we will randomly â€œdropâ€ with some probability (1p)a subset of neurons during each forward/backward pass (or equivalently, we would keep alive each neuron\n",
      "  Source:   {'source': 'cs224n_merged_notes.pdf', 'page': 39}\n",
      "  ROUGE-L:  baseline=0.1558  RAG=0.1628\n",
      "  Cosine:   baseline=0.7028  RAG=0.7946\n",
      "\n",
      "Q9: What is the difference between self-attention and cross-attention?\n",
      "  Baseline: Self-attentive refers to paying attention to oneself. Cross-attentive refers a person's attention to something other than oneself, such as a friend or family member. Therefore , cross-atttentive and self-atTENtive are the same thing.\n",
      "  RAG:      Self-Attention is the basis of the method. Position representations: Specify the sequence order, since self-attention is an unordered function of its inputs. Nonlinearities: Frequently implemented as a simple feed-forward network. Masking: In order to parallelize operations while not looking at the future. Keeps information about the future from â€œleakingâ€ to the past.\n",
      "  Source:   {'source': 'cs224n_transformers_2024.pdf', 'page': 43}\n",
      "  ROUGE-L:  baseline=0.1622  RAG=0.1957\n",
      "  Cosine:   baseline=0.3106  RAG=0.5549\n",
      "\n",
      "Q10: How does the encoder-decoder architecture work in sequence-to-sequence models?\n",
      "  Baseline: The encoder-decoder architecture works in sequence-to-sequence models by encrypting and decrypting a sequence. Therefore, the final answer should be encoder - decoder.\n",
      "  RAG:      Here, the encoder maps an input sequence of symbol representations to a sequence of continuous representations z= (z1;:::;z n). Given z, the decoder then generates an output sequence (y1;y m) of symbols one element at a time. At each step the model is auto-regressive, consuming the previously generated symbols as additional input when generating the next. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encode and decode\n",
      "  Source:   {'source': 'cs224n_merged_notes.pdf', 'page': 65}\n",
      "  ROUGE-L:  baseline=0.2333  RAG=0.3077\n",
      "  Cosine:   baseline=0.6344  RAG=0.6365\n"
     ]
    }
   ],
   "source": [
    "# Build results_df from already-computed results (no need to re-generate)\n",
    "results_df = pd.DataFrame([{\n",
    "    \"Question\": r[\"question\"][:60] + \"...\",\n",
    "    \"RAG ROUGE-L\": round(r[\"rag_rouge\"], 4),\n",
    "    \"Baseline ROUGE-L\": round(r[\"baseline_rouge\"], 4),\n",
    "    \"RAG Cosine\": round(r[\"rag_cosine\"], 4),\n",
    "    \"Baseline Cosine\": round(r[\"baseline_cosine\"], 4),\n",
    "    \"ROUGE Î”\": round(r[\"rag_rouge\"] - r[\"baseline_rouge\"], 4),\n",
    "    \"Cosine Î”\": round(r[\"rag_cosine\"] - r[\"baseline_cosine\"], 4),\n",
    "} for r in results])\n",
    "\n",
    "# Print answer comparison for each question\n",
    "print(\"=\" * 80)\n",
    "print(\"         RAG vs BASELINE â€” ANSWER COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "for i, r in enumerate(results):\n",
    "    print(f\"\\nQ{i+1}: {r['question']}\")\n",
    "    print(f\"  Baseline: {r['baseline_answer']}\")\n",
    "    print(f\"  RAG:      {r['rag_answer']}\")\n",
    "    print(f\"  Source:   {r['top_source']}\")\n",
    "    print(f\"  ROUGE-L:  baseline={r['baseline_rouge']:.4f}  RAG={r['rag_rouge']:.4f}\")\n",
    "    print(f\"  Cosine:   baseline={r['baseline_cosine']:.4f}  RAG={r['rag_cosine']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "89051aad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Question",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "RAG ROUGE-L",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Baseline ROUGE-L",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "RAG Cosine",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Baseline Cosine",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "ROUGE Î”",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Cosine Î”",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "8702a9ed-87a2-440e-a3b2-5abeb9a7b0bf",
       "rows": [
        [
         "0",
         "What is the key innovation of the Transformer architecture?...",
         "0.1143",
         "0.2892",
         "0.5129",
         "0.5439",
         "-0.1749",
         "-0.031"
        ],
        [
         "1",
         "How does multi-head attention work in Transformers?...",
         "0.2",
         "0.2059",
         "0.5322",
         "0.5812",
         "-0.0059",
         "-0.049"
        ],
        [
         "2",
         "What is backpropagation?...",
         "0.2162",
         "0.2222",
         "0.8461",
         "0.7927",
         "-0.006",
         "0.0533"
        ],
        [
         "3",
         "What is the purpose of positional encoding in Transformers?...",
         "0.1613",
         "0.2295",
         "0.7009",
         "0.6655",
         "-0.0682",
         "0.0354"
        ],
        [
         "4",
         "What is the vanishing gradient problem?...",
         "0.1765",
         "0.2",
         "0.802",
         "0.8175",
         "-0.0235",
         "-0.0156"
        ],
        [
         "5",
         "How do Convolutional Neural Networks process images?...",
         "0.2051",
         "0.1039",
         "0.5279",
         "-0.0435",
         "0.1012",
         "0.5714"
        ],
        [
         "6",
         "What is the softmax function and where is it used in attenti...",
         "0.2899",
         "0.2222",
         "0.7319",
         "0.7165",
         "0.0676",
         "0.0154"
        ],
        [
         "7",
         "What is dropout regularization and why is it used?...",
         "0.1628",
         "0.1558",
         "0.7946",
         "0.7028",
         "0.0069",
         "0.0918"
        ],
        [
         "8",
         "What is the difference between self-attention and cross-atte...",
         "0.1957",
         "0.1622",
         "0.5549",
         "0.3106",
         "0.0335",
         "0.2443"
        ],
        [
         "9",
         "How does the encoder-decoder architecture work in sequence-t...",
         "0.3077",
         "0.2333",
         "0.6365",
         "0.6344",
         "0.0744",
         "0.0021"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 10
       }
      },
      "text/html": [
       "\n",
       "  <div id=\"df-1d42521d-83e7-4785-a92e-f91268e38c61\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>RAG ROUGE-L</th>\n",
       "      <th>Baseline ROUGE-L</th>\n",
       "      <th>RAG Cosine</th>\n",
       "      <th>Baseline Cosine</th>\n",
       "      <th>ROUGE Î”</th>\n",
       "      <th>Cosine Î”</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the key innovation of the Transformer architecture?...</td>\n",
       "      <td>0.1143</td>\n",
       "      <td>0.2892</td>\n",
       "      <td>0.5129</td>\n",
       "      <td>0.5439</td>\n",
       "      <td>-0.1749</td>\n",
       "      <td>-0.0310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How does multi-head attention work in Transformers?...</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.2059</td>\n",
       "      <td>0.5322</td>\n",
       "      <td>0.5812</td>\n",
       "      <td>-0.0059</td>\n",
       "      <td>-0.0490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is backpropagation?...</td>\n",
       "      <td>0.2162</td>\n",
       "      <td>0.2222</td>\n",
       "      <td>0.8461</td>\n",
       "      <td>0.7927</td>\n",
       "      <td>-0.0060</td>\n",
       "      <td>0.0533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the purpose of positional encoding in Transformers?...</td>\n",
       "      <td>0.1613</td>\n",
       "      <td>0.2295</td>\n",
       "      <td>0.7009</td>\n",
       "      <td>0.6655</td>\n",
       "      <td>-0.0682</td>\n",
       "      <td>0.0354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the vanishing gradient problem?...</td>\n",
       "      <td>0.1765</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.8020</td>\n",
       "      <td>0.8175</td>\n",
       "      <td>-0.0235</td>\n",
       "      <td>-0.0156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How do Convolutional Neural Networks process images?...</td>\n",
       "      <td>0.2051</td>\n",
       "      <td>0.1039</td>\n",
       "      <td>0.5279</td>\n",
       "      <td>-0.0435</td>\n",
       "      <td>0.1012</td>\n",
       "      <td>0.5714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What is the softmax function and where is it used in attenti...</td>\n",
       "      <td>0.2899</td>\n",
       "      <td>0.2222</td>\n",
       "      <td>0.7319</td>\n",
       "      <td>0.7165</td>\n",
       "      <td>0.0676</td>\n",
       "      <td>0.0154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What is dropout regularization and why is it used?...</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.1558</td>\n",
       "      <td>0.7946</td>\n",
       "      <td>0.7028</td>\n",
       "      <td>0.0069</td>\n",
       "      <td>0.0918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What is the difference between self-attention and cross-atte...</td>\n",
       "      <td>0.1957</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.5549</td>\n",
       "      <td>0.3106</td>\n",
       "      <td>0.0335</td>\n",
       "      <td>0.2443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How does the encoder-decoder architecture work in sequence-t...</td>\n",
       "      <td>0.3077</td>\n",
       "      <td>0.2333</td>\n",
       "      <td>0.6365</td>\n",
       "      <td>0.6344</td>\n",
       "      <td>0.0744</td>\n",
       "      <td>0.0021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "      \n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1d42521d-83e7-4785-a92e-f91268e38c61')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "      \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-1d42521d-83e7-4785-a92e-f91268e38c61 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-1d42521d-83e7-4785-a92e-f91268e38c61');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "  \n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                                          Question  \\\n",
       "0   What is the key innovation of the Transformer architecture?...   \n",
       "1           How does multi-head attention work in Transformers?...   \n",
       "2                                      What is backpropagation?...   \n",
       "3   What is the purpose of positional encoding in Transformers?...   \n",
       "4                       What is the vanishing gradient problem?...   \n",
       "5          How do Convolutional Neural Networks process images?...   \n",
       "6  What is the softmax function and where is it used in attenti...   \n",
       "7            What is dropout regularization and why is it used?...   \n",
       "8  What is the difference between self-attention and cross-atte...   \n",
       "9  How does the encoder-decoder architecture work in sequence-t...   \n",
       "\n",
       "   RAG ROUGE-L  Baseline ROUGE-L  RAG Cosine  Baseline Cosine  ROUGE Î”  \\\n",
       "0       0.1143            0.2892      0.5129           0.5439  -0.1749   \n",
       "1       0.2000            0.2059      0.5322           0.5812  -0.0059   \n",
       "2       0.2162            0.2222      0.8461           0.7927  -0.0060   \n",
       "3       0.1613            0.2295      0.7009           0.6655  -0.0682   \n",
       "4       0.1765            0.2000      0.8020           0.8175  -0.0235   \n",
       "5       0.2051            0.1039      0.5279          -0.0435   0.1012   \n",
       "6       0.2899            0.2222      0.7319           0.7165   0.0676   \n",
       "7       0.1628            0.1558      0.7946           0.7028   0.0069   \n",
       "8       0.1957            0.1622      0.5549           0.3106   0.0335   \n",
       "9       0.3077            0.2333      0.6365           0.6344   0.0744   \n",
       "\n",
       "   Cosine Î”  \n",
       "0   -0.0310  \n",
       "1   -0.0490  \n",
       "2    0.0533  \n",
       "3    0.0354  \n",
       "4   -0.0156  \n",
       "5    0.5714  \n",
       "6    0.0154  \n",
       "7    0.0918  \n",
       "8    0.2443  \n",
       "9    0.0021  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d6c2e400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "           EVALUATION SUMMARY\n",
      "============================================================\n",
      "\n",
      "  ROUGE-L    â†’  Baseline: 0.2024  |  RAG: 0.2030  |  Î”: +0.0005\n",
      "  Cosine Sim â†’  Baseline: 0.5722  |  RAG: 0.6640  |  Î”: +0.0918\n",
      "\n",
      "  RAG wins on ROUGE-L:    5/10 questions\n",
      "  RAG wins on Cosine Sim: 7/10 questions\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"           EVALUATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "avg_rag_rouge = results_df[\"RAG ROUGE-L\"].mean()\n",
    "avg_base_rouge = results_df[\"Baseline ROUGE-L\"].mean()\n",
    "avg_rag_cos = results_df[\"RAG Cosine\"].mean()\n",
    "avg_base_cos = results_df[\"Baseline Cosine\"].mean()\n",
    "\n",
    "print(f\"\\n  ROUGE-L    â†’  Baseline: {avg_base_rouge:.4f}  |  RAG: {avg_rag_rouge:.4f}  |  Î”: {avg_rag_rouge - avg_base_rouge:+.4f}\")\n",
    "print(f\"  Cosine Sim â†’  Baseline: {avg_base_cos:.4f}  |  RAG: {avg_rag_cos:.4f}  |  Î”: {avg_rag_cos - avg_base_cos:+.4f}\")\n",
    "\n",
    "rouge_wins = sum(1 for r in results if r[\"rag_rouge\"] > r[\"baseline_rouge\"])\n",
    "cosine_wins = sum(1 for r in results if r[\"rag_cosine\"] > r[\"baseline_cosine\"])\n",
    "print(f\"\\n  RAG wins on ROUGE-L:    {rouge_wins}/{len(results)} questions\")\n",
    "print(f\"  RAG wins on Cosine Sim: {cosine_wins}/{len(results)} questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b7a0092e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval_accuracy(test_data, top_k=5):\n",
    "    hits = 0\n",
    "    total = 0\n",
    "\n",
    "    for item in test_data:\n",
    "        q = item[\"question\"]\n",
    "        expected_keywords = item[\"reference\"].split()[:3]  # simple keyword proxy\n",
    "\n",
    "        retrieved = retrieve(q, top_k)\n",
    "        combined_text = \" \".join([doc[\"chunk\"] for doc in retrieved])\n",
    "\n",
    "        if any(word.lower() in combined_text.lower() for word in expected_keywords):\n",
    "            hits += 1\n",
    "\n",
    "        total += 1\n",
    "\n",
    "    return hits / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "82397ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate Retrieval Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Approximate Retrieval Accuracy:\", retrieval_accuracy(test_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
